{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc6a83a-7783-4729-9707-22a53ad4cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import mmap\n",
    "import tempfile\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d51233-5453-48e9-8a19-25c77970e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import faiss\n",
    "from datasketch import MinHashLSH, MinHash\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8270e9c-c87f-47b7-881a-db0265c13cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "    from transformers import AutoTokenizer\n",
    "    ONNX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ONNX libraries not available. Install with: pip install onnx onnxruntime optimum[onnxruntime]\")\n",
    "    ONNX_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ece8382-10b2-407f-a350-6f17f339372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor CPU, memory, and time performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.start_memory = None\n",
    "        self.peak_memory = 0\n",
    "        \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        self.peak_memory = self.start_memory\n",
    "        \n",
    "    def update_peak_memory(self):\n",
    "        current_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        self.peak_memory = max(self.peak_memory, current_memory)\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = time.time()\n",
    "        self.update_peak_memory()\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'execution_time': self.end_time - self.start_time if self.end_time else 0,\n",
    "            'memory_used': self.peak_memory - self.start_memory,\n",
    "            'peak_memory_mb': self.peak_memory\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb152b3-5fab-4647-9f0c-fbfc71584152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"Generate synthetic dataset for testing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_product_dataset(n_samples=1000):\n",
    "        \"\"\"Create a product catalog dataset\"\"\"\n",
    "        categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', 'Toys']\n",
    "        brands = ['Apple', 'Samsung', 'Nike', 'Adidas', 'Sony', 'Microsoft', 'Amazon', 'Generic']\n",
    "        \n",
    "        products = []\n",
    "        for i in range(n_samples):\n",
    "            category = np.random.choice(categories)\n",
    "            brand = np.random.choice(brands)\n",
    "            \n",
    "            if category == 'Electronics':\n",
    "                items = ['Phone', 'Laptop', 'Tablet', 'Headphones', 'Camera', 'Smart Watch']\n",
    "                features = ['4K', 'Wireless', 'Bluetooth', 'Fast Charging', 'Waterproof']\n",
    "            elif category == 'Clothing':\n",
    "                items = ['T-Shirt', 'Jeans', 'Shoes', 'Jacket', 'Dress', 'Hat']\n",
    "                features = ['Cotton', 'Comfortable', 'Stylish', 'Durable', 'Breathable']\n",
    "            else:\n",
    "                items = ['Book', 'Novel', 'Guide', 'Manual', 'Dictionary']\n",
    "                features = ['Bestseller', 'Educational', 'Popular', 'New Release', 'Classic']\n",
    "            \n",
    "            item = np.random.choice(items)\n",
    "            feature = np.random.choice(features)\n",
    "            \n",
    "            # Create variations of the same product\n",
    "            if np.random.random() < 0.3:  # 30% chance of creating similar product\n",
    "                name = f\"{brand} {item} {feature} Pro\"\n",
    "            else:\n",
    "                name = f\"{brand} {item} {feature}\"\n",
    "                \n",
    "            description = f\"High quality {item.lower()} from {brand} with {feature.lower()} technology\"\n",
    "            \n",
    "            products.append({\n",
    "                'id': f'PROD_{i:04d}',\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'category': category,\n",
    "                'brand': brand,\n",
    "                'price': np.random.uniform(10, 1000)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34826bd9-53e3-4f1f-91c0-18baca2391bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "class BaselineFuzzyMatcher:\n",
    "    \"\"\"Baseline implementation without optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'sentence_transformer': SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "            'tfidf': TfidfVectorizer(max_features=1000, stop_words='english'),\n",
    "        }\n",
    "        self.embeddings = {}\n",
    "        \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Fit models on texts\"\"\"\n",
    "        # Sentence transformer embeddings\n",
    "        self.embeddings['sentence_transformer'] = self.models['sentence_transformer'].encode(texts)\n",
    "        \n",
    "        # TF-IDF embeddings\n",
    "        tfidf_matrix = self.models['tfidf'].fit_transform(texts)\n",
    "        self.embeddings['tfidf'] = tfidf_matrix.toarray()\n",
    "        \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches for query texts\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Sentence transformer matching\n",
    "        query_embeddings = self.models['sentence_transformer'].encode(query_texts)\n",
    "        similarities = cosine_similarity(query_embeddings, self.embeddings['sentence_transformer'])\n",
    "        results['sentence_transformer'] = [\n",
    "            np.argsort(sim)[-top_k:][::-1] for sim in similarities\n",
    "        ]\n",
    "        \n",
    "        # TF-IDF matching\n",
    "        query_tfidf = self.models['tfidf'].transform(query_texts).toarray()\n",
    "        similarities = cosine_similarity(query_tfidf, self.embeddings['tfidf'])\n",
    "        results['tfidf'] = [\n",
    "            np.argsort(sim)[-top_k:][::-1] for sim in similarities\n",
    "        ]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d197e4f-85f9-4a81-a53f-e9aac871dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNXOptimizedMatcher:\n",
    "    \"\"\"ONNX + Quantization optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_path = None\n",
    "        self.tokenizer = None\n",
    "        self.ort_session = None\n",
    "        self.embeddings = {}\n",
    "        \n",
    "    def convert_to_onnx(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"Convert model to ONNX format\"\"\"\n",
    "        if not ONNX_AVAILABLE:\n",
    "            raise ImportError(\"ONNX libraries not available\")\n",
    "            \n",
    "        try:\n",
    "            # Load tokenizer and model with ONNX export\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "                model_name, \n",
    "                export=True,\n",
    "                # providers=[\"CPUExecutionProvider\"]  # optional here\n",
    "            )\n",
    "            \n",
    "            model_dir = f\"./onnx_{model_name.replace('/', '_')}\"\n",
    "            model.save_pretrained(model_dir)\n",
    "            self.model_path = f\"{model_dir}/model.onnx\"\n",
    "            \n",
    "            self.ort_session = ort.InferenceSession(\n",
    "                self.model_path,\n",
    "                providers=['CPUExecutionProvider']\n",
    "            )\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ONNX conversion failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]):\n",
    "        if not self.ort_session:\n",
    "            return None\n",
    "            \n",
    "        embeddings = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors=\"np\",\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            outputs = self.ort_session.run(\n",
    "                None, \n",
    "                {\n",
    "                    \"input_ids\": inputs[\"input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "                    \"token_type_ids\": inputs[\"token_type_ids\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            batch_embeds = outputs[0].mean(axis=1)\n",
    "            embeddings.extend(batch_embeds)  # correct way\n",
    "            \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        self.embeddings['onnx'] = self.encode_texts(texts)\n",
    "        \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        query_embeddings = self.encode_texts(query_texts)\n",
    "        if query_embeddings is None:\n",
    "            return {}\n",
    "        similarities = cosine_similarity(query_embeddings, self.embeddings['onnx'])\n",
    "        return {\n",
    "            'onnx': [np.argsort(sim)[-top_k:][::-1] for sim in similarities]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68f78c5d-7a0d-48ac-ac25-ec6f650da492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedSimilarityMatcher:\n",
    "    \"\"\"Precomputed similarity matrices optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.similarity_matrices = {}\n",
    "        self.texts = []\n",
    "        self.embeddings = {}\n",
    "        \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Precompute similarity matrices\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Use lightweight TF-IDF for precomputation\n",
    "        vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Precompute full similarity matrix\n",
    "        self.similarity_matrices['tfidf'] = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Store for query processing\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using precomputed matrices\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for query in query_texts:\n",
    "            # Transform query\n",
    "            query_vec = self.vectorizer.transform([query])\n",
    "            \n",
    "            # Compute similarity with all documents\n",
    "            similarities = cosine_similarity(query_vec, self.vectorizer.transform(self.texts))[0]\n",
    "            \n",
    "            # Get top matches\n",
    "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "            results.append(top_indices)\n",
    "            \n",
    "        return {'precomputed': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0745fe-b6bb-42e7-861e-289dc1fb7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHMatcher:\n",
    "    \"\"\"Locality Sensitive Hashing optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5, num_perm=128):\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        self.minhashes = {}\n",
    "        self.texts = []\n",
    "        \n",
    "    def _create_minhash(self, text: str):\n",
    "        \"\"\"Create MinHash for text\"\"\"\n",
    "        minhash = MinHash(num_perm=self.num_perm)\n",
    "        # Simple tokenization\n",
    "        tokens = text.lower().split()\n",
    "        for token in tokens:\n",
    "            minhash.update(token.encode('utf8'))\n",
    "        return minhash\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Build LSH index\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            minhash = self._create_minhash(text)\n",
    "            self.minhashes[i] = minhash\n",
    "            self.lsh.insert(i, minhash)\n",
    "    \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using LSH\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for query in query_texts:\n",
    "            query_minhash = self._create_minhash(query)\n",
    "            candidates = list(self.lsh.query(query_minhash))\n",
    "            \n",
    "            # If not enough candidates, pad with random indices\n",
    "            if len(candidates) < top_k:\n",
    "                remaining = top_k - len(candidates)\n",
    "                available_indices = set(range(len(self.texts))) - set(candidates)\n",
    "                candidates.extend(np.random.choice(list(available_indices), \n",
    "                                                 min(remaining, len(available_indices)), \n",
    "                                                 replace=False))\n",
    "            \n",
    "            results.append(candidates[:top_k])\n",
    "            \n",
    "        return {'lsh': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaaf4f44-f6b1-41ff-abe4-c1b856eb2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMappedEmbeddings:\n",
    "    \"\"\"Memory-mapped embeddings for large datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_file, shape, dtype=np.float32):\n",
    "        self.file = open(embeddings_file, 'r+b')\n",
    "        self.mmap = mmap.mmap(self.file.fileno(), 0)\n",
    "        self.embeddings = np.frombuffer(self.mmap, dtype=dtype).reshape(shape)\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx].copy()\n",
    "    \n",
    "    def get_batch(self, indices):\n",
    "        \"\"\"Get multiple embeddings efficiently\"\"\"\n",
    "        return np.array([self.embeddings[i] for i in indices])\n",
    "    \n",
    "    def close(self):\n",
    "        if hasattr(self, 'embeddings'):\n",
    "            self.embeddings = None\n",
    "        if hasattr(self, 'mmap'):\n",
    "            self.mmap.close()\n",
    "        if hasattr(self, 'file'):\n",
    "            self.file.close()\n",
    "\n",
    "class MemoryMappedMatcher:\n",
    "    \"\"\"Memory mapping optimization for large embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mmap_embeddings = None\n",
    "        self.temp_files = []\n",
    "        self.texts = []\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Create memory-mapped embeddings\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.model.encode(texts)\n",
    "        \n",
    "        # Save to temporary file for memory mapping\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mmap')\n",
    "        temp_file.close()\n",
    "        self.temp_files.append(temp_file.name)\n",
    "        \n",
    "        # Write embeddings to file\n",
    "        embeddings.astype(np.float32).tofile(temp_file.name)\n",
    "        \n",
    "        # Create memory-mapped access\n",
    "        self.mmap_embeddings = MemoryMappedEmbeddings(\n",
    "            temp_file.name, \n",
    "            embeddings.shape, \n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using memory-mapped embeddings\"\"\"\n",
    "        if not self.mmap_embeddings:\n",
    "            return {}\n",
    "        \n",
    "        # Encode queries normally (small dataset)\n",
    "        query_embeddings = self.model.encode(query_texts)\n",
    "        \n",
    "        results = []\n",
    "        for query_emb in query_embeddings:\n",
    "            similarities = []\n",
    "            \n",
    "            # Process in batches to avoid loading all embeddings\n",
    "            batch_size = 100\n",
    "            for i in range(0, self.mmap_embeddings.shape[0], batch_size):\n",
    "                end_idx = min(i + batch_size, self.mmap_embeddings.shape[0])\n",
    "                batch_indices = list(range(i, end_idx))\n",
    "                \n",
    "                # Get batch of embeddings from memory map\n",
    "                batch_embeddings = self.mmap_embeddings.get_batch(batch_indices)\n",
    "                \n",
    "                # Compute similarities for this batch\n",
    "                batch_similarities = cosine_similarity([query_emb], batch_embeddings)[0]\n",
    "                similarities.extend([(i + j, sim) for j, sim in enumerate(batch_similarities)])\n",
    "            \n",
    "            # Sort by similarity and get top_k\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_indices = [idx for idx, _ in similarities[:top_k]]\n",
    "            results.append(top_indices)\n",
    "        \n",
    "        return {'memory_mapped': results}\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup temporary files\"\"\"\n",
    "        if hasattr(self, 'mmap_embeddings') and self.mmap_embeddings:\n",
    "            self.mmap_embeddings.close()\n",
    "        \n",
    "        for temp_file in self.temp_files:\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2beb2b42-6cd8-4ee1-9156-7c9fcdbaaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEmbeddingMatcher:\n",
    "    \"\"\"Sparse embeddings optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, sparsity_threshold=0.1):\n",
    "        self.sparsity_threshold = sparsity_threshold\n",
    "        self.sparse_embeddings = None\n",
    "        self.texts = []\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def _sparsify(self, embeddings, threshold):\n",
    "        \"\"\"Convert dense embeddings to sparse by thresholding\"\"\"\n",
    "        sparse = csr_matrix(embeddings)\n",
    "        sparse.data[np.abs(sparse.data) < threshold] = 0\n",
    "        sparse.eliminate_zeros()\n",
    "        return sparse\n",
    "    \n",
    "    def _sparse_cosine_similarity(self, A, B):\n",
    "        \"\"\"Compute cosine similarity between sparse matrices\"\"\"\n",
    "        # A·B / (||A|| * ||B||)\n",
    "        dot_product = A @ B.T\n",
    "        \n",
    "        # Compute norms\n",
    "        A_norm = np.sqrt(A.multiply(A).sum(axis=1))\n",
    "        B_norm = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        A_norm = np.maximum(A_norm, 1e-8)\n",
    "        B_norm = np.maximum(B_norm, 1e-8)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = dot_product.multiply(1 / A_norm).multiply(1 / B_norm.T)\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Create sparse embeddings\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Generate dense embeddings\n",
    "        dense_embeddings = self.model.encode(texts)\n",
    "        \n",
    "        # Convert to sparse\n",
    "        self.sparse_embeddings = self._sparsify(dense_embeddings, self.sparsity_threshold)\n",
    "        \n",
    "        print(f\"Sparsity: {1 - self.sparse_embeddings.nnz / np.prod(self.sparse_embeddings.shape):.2%}\")\n",
    "    \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using sparse embeddings\"\"\"\n",
    "        if self.sparse_embeddings is None:\n",
    "            return {}\n",
    "        \n",
    "        # Encode and sparsify queries\n",
    "        query_dense = self.model.encode(query_texts)\n",
    "        query_sparse = self._sparsify(query_dense, self.sparsity_threshold)\n",
    "        \n",
    "        # Compute sparse cosine similarity\n",
    "        similarities = self._sparse_cosine_similarity(query_sparse, self.sparse_embeddings)\n",
    "        \n",
    "        # Convert to dense for easier processing\n",
    "        similarities = similarities.toarray()\n",
    "        \n",
    "        results = []\n",
    "        for sim_row in similarities:\n",
    "            top_indices = np.argsort(sim_row)[-top_k:][::-1]\n",
    "            results.append(top_indices)\n",
    "        \n",
    "        return {'sparse_embeddings': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d54409a1-b7b0-4390-b7fb-621e037d63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessingMatcher:\n",
    "    \"\"\"Batch processing optimization for efficient computation\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64):\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = None\n",
    "        self.texts = []\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Create embeddings using batch processing\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Process in batches to manage memory\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using batch processing\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return {}\n",
    "        \n",
    "        # Process queries in batches\n",
    "        all_results = []\n",
    "        for i in range(0, len(query_texts), min(self.batch_size, 16)):  # Smaller batch for queries\n",
    "            batch_queries = query_texts[i:i + min(self.batch_size, 16)]\n",
    "            batch_embeddings = self.model.encode(batch_queries, show_progress_bar=False)\n",
    "            \n",
    "            # Compute similarities for this batch\n",
    "            similarities = cosine_similarity(batch_embeddings, self.embeddings)\n",
    "            \n",
    "            # Get top matches for each query in batch\n",
    "            for sim_row in similarities:\n",
    "                top_indices = np.argsort(sim_row)[-top_k:][::-1]\n",
    "                all_results.append(top_indices)\n",
    "        \n",
    "        return {'batch_processing': all_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce3540a-bf27-40af-83f2-494971e65798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltimateCombinedMatcher:\n",
    "    \"\"\"Combined optimization using all six strategies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.matchers = {}\n",
    "        self.texts = []\n",
    "        \n",
    "        # Initialize all matchers\n",
    "        if ONNX_AVAILABLE:\n",
    "            self.matchers['onnx'] = ONNXOptimizedMatcher()\n",
    "        self.matchers['precomputed'] = PrecomputedSimilarityMatcher()\n",
    "        self.matchers['lsh'] = LSHMatcher()\n",
    "        self.matchers['memory_mapped'] = MemoryMappedMatcher()\n",
    "        self.matchers['sparse_embeddings'] = SparseEmbeddingMatcher()\n",
    "        self.matchers['batch_processing'] = BatchProcessingMatcher()\n",
    "    \n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Fit all optimized matchers\"\"\"\n",
    "        self.texts = texts\n",
    "        \n",
    "        for name, matcher in self.matchers.items():\n",
    "            try:\n",
    "                if name == 'onnx' and hasattr(matcher, 'convert_to_onnx'):\n",
    "                    if matcher.convert_to_onnx():\n",
    "                        matcher.fit(texts)\n",
    "                    else:\n",
    "                        print(f\"Skipping {name} due to conversion failure\")\n",
    "                        continue\n",
    "                else:\n",
    "                    matcher.fit(texts)\n",
    "                print(f\"✅ {name} fitted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {name} failed: {e}\")\n",
    "    \n",
    "    def find_matches(self, query_texts: List[str], top_k=5):\n",
    "        \"\"\"Find matches using ensemble of all methods\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        # Get results from each method\n",
    "        for name, matcher in self.matchers.items():\n",
    "            try:\n",
    "                results = matcher.find_matches(query_texts, top_k)\n",
    "                all_results.update(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Advanced ensemble voting with weights\n",
    "        ensemble_results = []\n",
    "        method_weights = {\n",
    "            'onnx': 0.2,\n",
    "            'precomputed': 0.15,\n",
    "            'lsh': 0.15,\n",
    "            'memory_mapped': 0.2,\n",
    "            'sparse_embeddings': 0.15,\n",
    "            'batch_processing': 0.15\n",
    "        }\n",
    "        \n",
    "        for i in range(len(query_texts)):\n",
    "            vote_scores = {}\n",
    "            \n",
    "            for method, results in all_results.items():\n",
    "                if i < len(results):\n",
    "                    weight = method_weights.get(method, 0.1)\n",
    "                    for rank, idx in enumerate(results[i][:top_k]):\n",
    "                        # Higher rank = higher score, weighted by method importance\n",
    "                        score = (top_k - rank) * weight\n",
    "                        vote_scores[idx] = vote_scores.get(idx, 0) + score\n",
    "            \n",
    "            # Sort by weighted score and take top_k\n",
    "            sorted_candidates = sorted(vote_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            ensemble_results.append([idx for idx, _ in sorted_candidates[:top_k]])\n",
    "        \n",
    "        all_results['ultimate_ensemble'] = ensemble_results\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11073e99-5f09-4c63-9d76-83ab456fda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_ndarray(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_ndarray(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_ndarray(i) for i in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4807aa-c0e3-4dd3-bccc-c9a7117b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psutil\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "def run_comprehensive_comparison(dataset_path=\"alethia/notebooks/Amazon-Google\", split=\"train.csv\"):\n",
    "    from alethia.datasets import DatasetLoader\n",
    "\n",
    "    print(\"🚀 COMPREHENSIVE ML FUZZY MATCHING OPTIMIZATION COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"📊 Loading real benchmark dataset...\")\n",
    "    \n",
    "    # Load real data\n",
    "    texts, query_texts, ground_truth = DatasetLoader.load_amazon_google(\n",
    "        dataset_path=dataset_path, split=split\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(texts)} items\")\n",
    "    print(f\"Query size: {len(query_texts)} queries\\n\")\n",
    "\n",
    "    matchers = {\n",
    "        'baseline': BaselineFuzzyMatcher(),\n",
    "        'onnx': ONNXOptimizedMatcher(),\n",
    "        'precomputed': PrecomputedSimilarityMatcher(),\n",
    "        'lsh': LSHMatcher(),\n",
    "        'memory_mapped': MemoryMappedMatcher(),\n",
    "        'sparse_embeddings': SparseEmbeddingMatcher(),\n",
    "        'batch_processing': BatchProcessingMatcher(),\n",
    "        'ultimate_combined': UltimateCombinedMatcher()\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, matcher in matchers.items():\n",
    "        print(f\"🔧 Testing {name.upper()} approach...\")\n",
    "\n",
    "        start = time.time()\n",
    "        tracemalloc.start()\n",
    "\n",
    "        try:\n",
    "            if name == 'onnx':\n",
    "                if not matcher.convert_to_onnx():\n",
    "                    raise RuntimeError(\"ONNX conversion failed\")\n",
    "            matcher.fit(texts)\n",
    "            matches = matcher.find_matches(query_texts, top_k=5)\n",
    "\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            end = time.time()\n",
    "\n",
    "            avg_matches = np.mean([len(m) for m in list(matches.values())[0]])\n",
    "\n",
    "            results[name] = {\n",
    "                'success': True,\n",
    "                'time': round(end - start, 4),\n",
    "                'memory': round(current / 1024 / 1024, 2),\n",
    "                'peak_memory': round(peak / 1024 / 1024, 2),\n",
    "                'matches': matches,\n",
    "                'avg_matches': avg_matches\n",
    "            }\n",
    "\n",
    "            print(f\"   ✅ Time: {results[name]['time']}s\")\n",
    "            print(f\"   ✅ Memory: {results[name]['memory']}MB\")\n",
    "            print(f\"   ✅ Peak Memory: {results[name]['peak_memory']}MB\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            end = time.time()\n",
    "            results[name] = {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'time': round(end - start, 4),\n",
    "                'memory': round(current / 1024 / 1024, 2),\n",
    "                'peak_memory': round(peak / 1024 / 1024, 2),\n",
    "            }\n",
    "            print(f\"   ❌ Failed: {e}\\n\")\n",
    "\n",
    "        tracemalloc.stop()\n",
    "\n",
    "    # Generate Rankings\n",
    "    success_results = {k: v for k, v in results.items() if v['success']}\n",
    "    print(\"\\n📈 COMPREHENSIVE COMPARISON RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Rankings\n",
    "    speed_ranking = sorted(success_results.items(), key=lambda x: x[1]['time'])\n",
    "    memory_ranking = sorted(success_results.items(), key=lambda x: x[1]['memory'])\n",
    "\n",
    "    print(\"\\n🏆 PERFORMANCE RANKINGS:\\n----------------------------------------\")\n",
    "    print(\"⚡ SPEED RANKING (fastest to slowest):\")\n",
    "    for i, (name, r) in enumerate(speed_ranking, 1):\n",
    "        print(f\"   {i}. {name.upper():<20}: {r['time']}s\")\n",
    "\n",
    "    print(\"\\n🧠 MEMORY RANKING (most to least efficient):\")\n",
    "    for i, (name, r) in enumerate(memory_ranking, 1):\n",
    "        print(f\"   {i}. {name.upper():<20}: {r['memory']}MB\")\n",
    "\n",
    "    # Match sample output\n",
    "    print(\"\\n🎯 MATCH QUALITY ANALYSIS:\\n----------------------------------------\")\n",
    "    for name, r in success_results.items():\n",
    "        print(f\"{name.upper():<20}: {r['avg_matches']:.2f} avg matches\")\n",
    "        sample_match = list(r['matches'].values())[0][0:5] if r['matches'] else []\n",
    "        print(f\"  Sample match: {sample_match}...\")\n",
    "\n",
    "    # Strategy recommendations\n",
    "    best_speed = speed_ranking[0][0]\n",
    "    best_memory = memory_ranking[0][0]\n",
    "\n",
    "    print(\"\\n🌟 RECOMMENDATIONS:\\n----------------------------------------\")\n",
    "    print(f\"• For SPEED: {best_speed.upper()} ({success_results[best_speed]['time']}s)\")\n",
    "    print(f\"• For MEMORY: {best_memory.upper()} ({success_results[best_memory]['memory']}MB)\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6086f2fd-941d-4060-85a0-d849deb68740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional utility functions for the benchmark\n",
    "def compare_specific_strategies(strategy_names, dataset_size=1000):\n",
    "    \"\"\"Compare only specific strategies with custom dataset size\"\"\"\n",
    "    \n",
    "    print(f\"🎯 TARGETED STRATEGY COMPARISON\")\n",
    "    print(f\"Strategies: {', '.join(strategy_names)}\")\n",
    "    print(f\"Dataset size: {dataset_size}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate smaller dataset\n",
    "    df = DatasetGenerator.create_product_dataset(n_samples=dataset_size)\n",
    "    texts = (df['name'] + ' ' + df['description']).tolist()\n",
    "    \n",
    "    query_texts = [\n",
    "        \"Apple iPhone wireless\",\n",
    "        \"Nike running shoes\",\n",
    "        \"Samsung TV 4K\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize only selected matchers\n",
    "    all_matchers = {\n",
    "        'baseline': BaselineFuzzyMatcher(),\n",
    "        'onnx': ONNXOptimizedMatcher() if ONNX_AVAILABLE else None,\n",
    "        'precomputed': PrecomputedSimilarityMatcher(),\n",
    "        'lsh': LSHMatcher(),\n",
    "        'memory_mapped': MemoryMappedMatcher(),\n",
    "        'sparse_embeddings': SparseEmbeddingMatcher(),\n",
    "        'batch_processing': BatchProcessingMatcher(),\n",
    "        'ultimate_combined': UltimateCombinedMatcher()\n",
    "    }\n",
    "    \n",
    "    selected_matchers = {name: matcher for name, matcher in all_matchers.items() \n",
    "                        if name in strategy_names and matcher is not None}\n",
    "    \n",
    "    if not selected_matchers:\n",
    "        print(\"❌ No valid strategies found!\")\n",
    "        return {}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, matcher in selected_matchers.items():\n",
    "        print(f\"🔧 Testing {name.upper()}...\")\n",
    "        monitor = PerformanceMonitor()\n",
    "        \n",
    "        try:\n",
    "            monitor.start()\n",
    "            matcher.fit(texts)\n",
    "            matches = matcher.find_matches(query_texts, top_k=3)\n",
    "            monitor.stop()\n",
    "            \n",
    "            stats = monitor.get_stats()\n",
    "            results[name] = {\n",
    "                'execution_time': stats['execution_time'],\n",
    "                'memory_used': stats['memory_used'],\n",
    "                'matches': len(matches),\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"   ✅ Time: {stats['execution_time']:.2f}s, Memory: {stats['memory_used']:.1f}MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed: {str(e)}\")\n",
    "            results[name] = {'success': False, 'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def benchmark_scaling_performance():\n",
    "    \"\"\"Test how strategies scale with dataset size\"\"\"\n",
    "    \n",
    "    print(\"📈 SCALING PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    dataset_sizes = [100, 500, 1000, 2000]\n",
    "    strategies = ['baseline', 'lsh', 'sparse_embeddings']\n",
    "    \n",
    "    scaling_results = {}\n",
    "    \n",
    "    for size in dataset_sizes:\n",
    "        print(f\"\\n📊 Testing with {size} samples...\")\n",
    "        scaling_results[size] = compare_specific_strategies(strategies, size)\n",
    "    \n",
    "    # Analyze scaling trends\n",
    "    print(f\"\\n📈 SCALING ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n{strategy.upper()} Scaling:\")\n",
    "        times = []\n",
    "        memories = []\n",
    "        \n",
    "        for size in dataset_sizes:\n",
    "            if size in scaling_results and strategy in scaling_results[size]:\n",
    "                result = scaling_results[size][strategy]\n",
    "                if result['success']:\n",
    "                    times.append(result['execution_time'])\n",
    "                    memories.append(result['memory_used'])\n",
    "                    print(f\"  {size:>4} samples: {result['execution_time']:.2f}s, {result['memory_used']:.1f}MB\")\n",
    "        \n",
    "        # Calculate scaling factor\n",
    "        if len(times) >= 2:\n",
    "            time_scaling = times[-1] / times[0] if times[0] > 0 else float('inf')\n",
    "            memory_scaling = memories[-1] / memories[0] if memories[0] > 0 else float('inf')\n",
    "            print(f\"  Scaling factor: Time {time_scaling:.2f}x, Memory {memory_scaling:.2f}x\")\n",
    "    \n",
    "    return scaling_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a233b5-b673-4192-ad2f-35b169acae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 TARGETED STRATEGY COMPARISON\n",
      "Strategies: baseline, lsh\n",
      "Dataset size: 500\n",
      "==================================================\n",
      "🔧 Testing BASELINE...\n",
      "   ✅ Time: 0.68s, Memory: 393.0MB\n",
      "🔧 Testing LSH...\n",
      "   ✅ Time: 0.64s, Memory: 1.2MB\n"
     ]
    }
   ],
   "source": [
    "# Test just a few strategies with smaller dataset\n",
    "quick_results = compare_specific_strategies(['baseline', 'lsh'], dataset_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca7f53b1-ea34-4491-beb3-033db54b99c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 TARGETED STRATEGY COMPARISON\n",
      "Strategies: baseline, lsh, sparse_embeddings\n",
      "Dataset size: 800\n",
      "==================================================\n",
      "🔧 Testing BASELINE...\n",
      "   ✅ Time: 0.33s, Memory: 3.6MB\n",
      "🔧 Testing LSH...\n",
      "   ✅ Time: 1.00s, Memory: 1.7MB\n",
      "🔧 Testing SPARSE_EMBEDDINGS...\n",
      "Sparsity: 94.84%\n",
      "   ✅ Time: 0.38s, Memory: 8.9MB\n"
     ]
    }
   ],
   "source": [
    "# Choose exactly what you want to test\n",
    "my_strategies = ['baseline', 'lsh', 'sparse_embeddings']  # Your choice\n",
    "my_dataset_size = 800  # Your choice\n",
    "custom_results = compare_specific_strategies(my_strategies, my_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dcf23cc-511a-44d1-a1c6-f21f876be88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdc15d-0934-43c3-9681-9853e909812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = run_comprehensive_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cecce9-2235-4165-b38d-852bd07833e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
