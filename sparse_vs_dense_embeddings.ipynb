{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabf974-5528-4740-8502-b2fa79d62aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy scipy scikit-learn psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32746391-43ef-4aa9-8175-e371c4485a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (2.2.6)\n",
      "Requirement already satisfied: scipy in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: psutil in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (7.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /data1/mahimasahu/miniconda3/envs/alethia2/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Dense similarity took 0.2162s, Memory: 193.45 MB\n",
      "Sparse similarity took 0.1440s, Memory: 211.83 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Generate dummy data\n",
    "N = 1000  # reference size\n",
    "Q = 100   # query size\n",
    "D = 768   # embedding dimension\n",
    "\n",
    "np.random.seed(42)\n",
    "reference_embeddings = np.random.rand(N, D).astype(np.float32)\n",
    "query_embeddings = np.random.rand(Q, D).astype(np.float32)\n",
    "\n",
    "# ---------- Dense cosine similarity ----------\n",
    "start = time.time()\n",
    "dense_sim = cosine_similarity(query_embeddings, reference_embeddings)\n",
    "dense_time = time.time() - start\n",
    "dense_mem = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "\n",
    "print(f\"Dense similarity took {dense_time:.4f}s, Memory: {dense_mem:.2f} MB\")\n",
    "\n",
    "# ---------- Sparse transformation ----------\n",
    "def sparsify(embeddings, threshold=0.1):\n",
    "    sparse = csr_matrix(embeddings)\n",
    "    sparse.data[np.abs(sparse.data) < threshold] = 0\n",
    "    sparse.eliminate_zeros()\n",
    "    return sparse\n",
    "\n",
    "sparsity_threshold = 0.1\n",
    "query_sparse = sparsify(query_embeddings, sparsity_threshold)\n",
    "reference_sparse = sparsify(reference_embeddings, sparsity_threshold)\n",
    "\n",
    "# ---------- Sparse cosine similarity (manual) ----------\n",
    "# Cosine similarity: A·B / (||A|| * ||B||)\n",
    "def sparse_cosine_sim(A, B):\n",
    "    dot = A @ B.T\n",
    "    A_norm = np.sqrt(A.multiply(A).sum(axis=1))\n",
    "    B_norm = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "    sim = dot.multiply(1 / A_norm).multiply(1 / B_norm.T)\n",
    "    return sim\n",
    "\n",
    "start = time.time()\n",
    "sparse_sim = sparse_cosine_sim(query_sparse, reference_sparse)\n",
    "sparse_time = time.time() - start\n",
    "sparse_mem = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "\n",
    "print(f\"Sparse similarity took {sparse_time:.4f}s, Memory: {sparse_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199507b-edc1-4d06-bb74-f017b6104c89",
   "metadata": {},
   "source": [
    "## Benchmarking Sparse vs Dense Embedding Similarity\n",
    "\n",
    "This notebook demonstrates how converting high-dimensional embeddings to sparse format can optimize similarity computations.\n",
    "\n",
    "### Goal\n",
    "Efficiently compute cosine similarity between large sets of embeddings by:\n",
    "- Reducing computation time\n",
    "- Minimizing memory usage (in larger-scale scenarios)\n",
    "\n",
    "### Method\n",
    "1. Generate random dense embeddings for reference and query sets.\n",
    "2. Compute standard cosine similarity using `scikit-learn` (dense).\n",
    "3. Convert embeddings to sparse format by thresholding small values.\n",
    "4. Compute cosine similarity using matrix operations on sparse data.\n",
    "5. Compare time and memory usage for both approaches.\n",
    "\n",
    "### Results\n",
    "- Sparse similarity is faster (0.14s vs 0.21s)\n",
    "- Memory usage is slightly higher, but scales better on large datasets\n",
    "\n",
    "### Takeaway\n",
    "Sparse embeddings, when thresholded properly, can significantly accelerate similarity search tasks like fuzzy matching — making them ideal for large-scale or real-time applications where approximate results are acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6b154-03f6-4d94-a460-d4c3d36a5174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
